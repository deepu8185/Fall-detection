# -*- coding: utf-8 -*-
"""Gait_project(final) (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FW1M5WY0Nyu5rl1ixO7opFmq0uycQ_Wu
"""

from google.colab import drive
drive.mount('/content/gdrive')

"""# Dataset Prepration"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.metrics import accuracy_score
import pickle

# Read normal and abnormal data
normal = pd.read_csv("/content/gdrive/MyDrive/gait_stabilize/gait_dataset/normal.csv")
abnormal = pd.read_csv("/content/gdrive/MyDrive/gait_stabilize/gait_dataset/abnormal.csv")
normal_1 = pd.read_csv("/content/gdrive/MyDrive/gait_stabilize/gait_dataset/normal_1.csv")
abnormal_1 = pd.read_csv("/content/gdrive/MyDrive/gait_stabilize/gait_dataset/abnormal_1.csv")

# concatenate normal data with normal_1 data
normal_data = pd.concat([normal,normal_1],axis=0,ignore_index = True)
abnormal_data = pd.concat([abnormal,abnormal_1],axis=0,ignore_index = True)

# display first five rows for normal data
normal_data.head()

# display first five rows for abnormal data
abnormal_data.head()

# information about dataset
normal_data.info()

# information about dataset
abnormal_data.info()

# statistics  about dataset
normal_data.describe()

# statistics  about dataset
abnormal_data.describe()

"""# Heat Map for normal"""

corr = normal_data.corr()
fig, ax = plt.subplots(figsize = (10,5))

ax = sns.heatmap(corr, annot = True, fmt = '.2f')

"""# Heat map for abnormal"""

corr = abnormal_data.corr()
fig, ax = plt.subplots(figsize = (10,5))

ax = sns.heatmap(corr, annot = True, fmt = '.2f')

"""# Comparision of Normal and Abnormal accelerometer data


"""

# Normal x, y and z
fig, ax = plt.subplots(4,1)
fig.set_figheight(7.5)
fig.set_figwidth(15)

fig.suptitle("Accelerometer Data", fontsize = 30)
fig.tight_layout()

ax[0].plot(normal_data['accel_x'], 'b')
ax[0].set_ylabel('x-axis', fontdict = {'size':20})
# ax[0].set_ylim(-5,5)

ax[1].plot(normal_data['accel_y'], 'r')
ax[1].set_ylabel('y-axis', fontdict = {'size':20})
# ax[1].set_ylim(-5,5)

ax[2].plot(normal_data['accel_z'], 'g')
ax[2].set_ylabel('z-axis', fontdict = {'size':20})
# ax[2].set_ylim(-5,5)

normal_data['accel_mag'] = (normal_data['accel_x'] ** 2 + normal_data['accel_y'] ** 2 + normal_data['accel_z'] ** 2) ** 0.5
ax[3].plot(normal_data['accel_mag'], 'brown')
ax[3].set_ylabel('magnitude', fontdict = {'size':20})

plt.show()

# Abnormal x, y and z
fig, ax = plt.subplots(4,1)
fig.set_figheight(7.5)
fig.set_figwidth(15)

fig.suptitle("Accelerometer Data", fontsize = 30)
fig.tight_layout()

ax[0].plot(abnormal_data['accel_x'], 'b')
ax[0].set_ylabel('x-axis', fontdict = {'size':20})
# ax[0].set_ylim(-5,5)

ax[1].plot(abnormal_data['accel_y'], 'r')
ax[1].set_ylabel('y-axis', fontdict = {'size':20})
# ax[1].set_ylim(-5,5)

ax[2].plot(abnormal_data['accel_z'], 'g')
ax[2].set_ylabel('z-axis', fontdict = {'size':20})
# ax[2].set_ylim(-5,5)

abnormal_data['accel_mag'] = (abnormal_data['accel_x'] ** 2 + abnormal_data['accel_y'] ** 2 + abnormal_data['accel_z'] ** 2) ** 0.5
ax[3].plot(abnormal_data['accel_mag'], 'brown')
ax[3].set_ylabel('magnitude', fontdict = {'size':20})

plt.show()

normal_data

from scipy.signal import find_peaks
fig, ax = plt.subplots(2,1)
fig.set_figheight(7.5)
fig.set_figwidth(15)

# fig.suptitle("Peak detection in Normal and Abnormal Magnitude", fontsize = 30)
fig.tight_layout()

npeaks, _ = find_peaks(normal_data['accel_mag'], height=7)

ax[0].plot(normal_data['accel_mag'], 'b')
ax[0].plot(npeaks, normal_data['accel_mag'][npeaks], "rx")
ax[0].set_ylabel('magnitude', fontdict = {'size':20})
ax[0].set_title('Peaks detection in Normal Magnitude')

abpeaks, _ = find_peaks(abnormal_data['accel_mag'], height=7)

ax[1].plot(abnormal_data['accel_mag'], 'b')
ax[1].plot(abpeaks, abnormal_data['accel_mag'][abpeaks], "rx")
ax[1].set_ylabel('magnitude', fontdict = {'size':20})
ax[1].set_title('Peaks detection in Abnormal Magnitude')

print('No. of Steps taken by Normal Person:', len(npeaks))
print('No. of Steps taken by AbNormal Person:', len(abpeaks))

"""

# Comparison of Normal and Abnormal gyroscope data

"""

# Normal x, y and z
fig, ax = plt.subplots(3,1)
fig.set_figheight(7.5)
fig.set_figwidth(15)

fig.suptitle("Gyroscope Data", fontsize = 30)
fig.tight_layout()

ax[0].plot(normal_data['gyro_x'], 'b')
ax[0].set_ylabel('x-axis', fontdict = {'size':20})
# ax[0].set_ylim(-5,5)

ax[1].plot(normal_data['gyro_y'], 'r')
ax[1].set_ylabel('y-axis', fontdict = {'size':20})
# ax[1].set_ylim(-5,5)

ax[2].plot(normal_data['gyro_z'], 'g')
ax[2].set_ylabel('z-axis', fontdict = {'size':20})
# ax[2].set_ylim(-5,5)

plt.show()

# Abnormal x, y and z
fig, ax = plt.subplots(3,1)
fig.set_figheight(7.5)
fig.set_figwidth(15)

fig.suptitle("Gyroscope Data", fontsize = 30)
fig.tight_layout()

ax[0].plot(abnormal_data['gyro_x'], 'b')
ax[0].set_ylabel('x-axis', fontdict = {'size':20})
# ax[0].set_ylim(-5,5)

ax[1].plot(abnormal_data['gyro_y'], 'r')
ax[1].set_ylabel('y-axis', fontdict = {'size':20})
# ax[1].set_ylim(-5,5)

ax[2].plot(abnormal_data['gyro_z'], 'g')
ax[2].set_ylabel('z-axis', fontdict = {'size':20})
# ax[2].set_ylim(-5,5)

plt.show()

"""# Parameters calculation for normal"""

import numpy as np
from scipy.signal import find_peaks
from scipy.signal import find_peaks, peak_widths, peak_prominences


# Find peaks in the acceleration magnitude
peaks, _ = find_peaks(normal_data['accel_mag'], height=4)

# Calculate step times in seconds
step_times = np.diff(peaks)

# Calculate step length
walking_speed = 1.42  # m/s, example average walking speed
step_length = walking_speed * step_times

# Calculate step velocity in m/s^2
step_velocity = step_length / step_times

# Calculate step count difference of each peaks
step_count = np.diff(peaks)

# Stride times in seconds
stride_time = np.diff(peaks) / 100

# Calculate stride length
time_between_samples = 1 / 100
stride_length = peaks[:-1] * time_between_samples


# Cadence: Cadence refers to the number of steps taken per unit of time (e.g., steps per minute). It can provide insights into the walking rhythm and pace.
cadence = 60 / step_times

# Calculate peak properties
peak_properties = peak_prominences(normal_data['accel_mag'], peaks)

# Extract peak properties
peak_prominences = peak_properties[0][:-1]

peak_widths = peak_widths(normal_data['accel_mag'], peaks)[0][:-1]


# Calculate mean and standard deviation of accelerometer x and y axes data for each step
accel_x_mean = [np.mean(normal_data['accel_x'][peaks[i]:peaks[i+1]]) for i in range(len(peaks)-1)]
accel_y_mean = [np.mean(normal_data['accel_y'][peaks[i]:peaks[i+1]]) for i in range(len(peaks)-1)]
accel_x_std = [np.std(normal_data['accel_x'][peaks[i]:peaks[i+1]]) for i in range(len(peaks)-1)]
accel_y_std = [np.std(normal_data['accel_y'][peaks[i]:peaks[i+1]]) for i in range(len(peaks)-1)]

# # Convert lists to 1D arrays
accel_x_mean_array = np.array(accel_x_mean)
accel_y_mean_array = np.array(accel_y_mean)
accel_x_std_array = np.array(accel_x_std)
accel_y_std_array = np.array(accel_y_std)

# concatenate all feature
normal_feature = np.vstack((step_times, step_length, step_velocity, step_count, stride_time, stride_length,cadence,peak_prominences,peak_widths,accel_x_mean_array,accel_y_mean_array,accel_x_std_array,accel_y_std_array,np.zeros(step_times.shape[0]))).T

feature_names = ['step_times', 'step_length', 'step_velocity', 'step_count', 'stride_time', 'stride_length',"cadence","peak_prominences","peak_widths","accel_x_mean_array","accel_y_mean_array","accel_x_std_array","accel_y_std_array",'class']
normal_df = pd.DataFrame(normal_feature, columns=feature_names)

"""# Parameters calculation for abnormal"""

import numpy as np
from scipy.signal import find_peaks
from scipy.signal import find_peaks, peak_widths, peak_prominences
# Find peaks in the acceleration magnitude
peaks, _ = find_peaks(abnormal_data['accel_mag'], height=4)

import numpy as np
from scipy.signal import find_peaks
from scipy.signal import find_peaks, peak_widths, peak_prominences
# Find peaks in the acceleration magnitude
peaks, _ = find_peaks(abnormal_data['accel_mag'], height=7)

# Calculate step times in seconds
step_times = np.diff(peaks)

# Calculate step length
walking_speed = 1.42  # m/s, example average walking speed
step_length = walking_speed * step_times

# Calculate step velocity in m/s^2
step_velocity = step_length / step_times

# Calculate step count difference of each peaks
step_count = np.diff(peaks)

# Stride times in seconds
stride_time = np.diff(peaks) / 100

# Calculate stride length
time_between_samples = 1 / 100
stride_length = peaks[:-1] * time_between_samples


# Cadence: Cadence refers to the number of steps taken per unit of time (e.g., steps per minute). It can provide insights into the walking rhythm and pace.
cadence = 60 / step_times

# Calculate peak properties
peak_properties = peak_prominences(abnormal_data['accel_mag'], peaks)

# Extract peak properties
peak_prominences = peak_properties[0][:-1]

peak_widths = peak_widths(abnormal_data['accel_mag'], peaks)[0][:-1]

# Calculate mean and standard deviation of accelerometer x and y axes data for each step
accel_x_mean = [np.mean(abnormal_data['accel_x'][peaks[i]:peaks[i+1]]) for i in range(len(peaks)-1)]
accel_y_mean = [np.mean(abnormal_data['accel_y'][peaks[i]:peaks[i+1]]) for i in range(len(peaks)-1)]
accel_x_std = [np.std(abnormal_data['accel_x'][peaks[i]:peaks[i+1]]) for i in range(len(peaks)-1)]
accel_y_std = [np.std(abnormal_data['accel_y'][peaks[i]:peaks[i+1]]) for i in range(len(peaks)-1)]
# # Convert lists to 1D arrays
accel_x_mean_array = np.array(accel_x_mean)
accel_y_mean_array = np.array(accel_y_mean)
accel_x_std_array = np.array(accel_x_std)
accel_y_std_array = np.array(accel_y_std)

# concatenate extracted parameters
abnormal_feature = np.vstack((step_times, step_length, step_velocity, step_count, stride_time, stride_length,cadence,peak_prominences,peak_widths,accel_x_mean_array,accel_y_mean_array,accel_x_std_array,accel_y_std_array,np.ones(step_times.shape[0]))).T

feature_names = ['step_times', 'step_length', 'step_velocity', 'step_count', 'stride_time', 'stride_length',"cadence","peak_prominences","peak_widths","accel_x_mean_array","accel_y_mean_array","accel_x_std_array","accel_y_std_array",'class']
abnormal_df = pd.DataFrame(abnormal_feature, columns=feature_names)

df_data = pd.concat([normal_df,abnormal_df],axis=0)
# df_data.to_csv("/content/gdrive/MyDrive/gait_stabilize/df_data.csv",index = None)

df_data = pd.read_csv("/content/gdrive/MyDrive/gait_stabilize/df_data.csv")

df_data

df_data["class"].value_counts().plot(kind = "bar", color=["gray","coral"])
plt.title("Number of Normal and Abnormal data")
plt.show()

# get correlations of each features in dataset
corrmat = df_data.corr()
plt.figure(figsize=(10,5))
# plot heat map
sns.heatmap(corrmat,annot=True,cmap="Blues")

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you have the dataset with features and target variable loaded in a DataFrame (e.g., df)


# Compute the correlation matrix between features and the target variable
correlation_matrix = df_data.corr()

# Extract the correlation scores of each feature with the target variable
target_correlation_scores = correlation_matrix['class']

# Sort the features based on their correlation scores (absolute values) in descending order
sorted_features = target_correlation_scores.abs().sort_values(ascending=False)

# Print the correlation scores
print("Correlation Scores with Target Variable:")
print(sorted_features)

# Plot a bar plot to visualize the correlation scores
plt.figure(figsize=(10, 6))
sns.barplot(x=sorted_features.index, y=sorted_features.values)
plt.title('Correlation Scores with Target Variable')
plt.xlabel('Features')
plt.ylabel('Correlation Score')
plt.xticks(rotation=90)
plt.show()

# there is no correlation for step velocity so drop from dataframe
df_data.drop("step_velocity",axis=1,inplace = True)

# separate feature and labels
X = df_data.iloc[:,:-1]
y = df_data.iloc[:,-1]

# data normaliation
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X)
pickle.dump(scaler,open("/content/gdrive/MyDrive/gait_stabilize/scaler_weight.p","wb"))
scaled_data = scaler.transform(X)

# data splits into training and testing part
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(scaled_data,y,test_size = 0.2,random_state = 42)

data = {'total data':[len(X_train),len(X_test),len(y_train),len(y_test)]}
pd.DataFrame.from_dict(data, orient='index',columns=["xtrain","xtest","ytrain","ytest"])

# Convert the y_train and y_test back to a DataFrame for visualization
train_data = pd.DataFrame({'Feature1': X_train[:, 0], 'Feature2': X_train[:, 1], 'Class': y_train})
test_data = pd.DataFrame({'Feature1': X_test[:, 0], 'Feature2': X_test[:, 1], 'Class': y_test})

train_color = ["red","red"]
test_color = ["cyan","cyan"]
# Plot a bar plot to visualize the class distribution in the training and testing sets
plt.figure(figsize=(10, 8))
sns.countplot(x='Class', data=train_data, palette=train_color, alpha=0.7, label='Training Set')
sns.countplot(x='Class', data=test_data, palette=test_color, alpha=0.7, label='Testing Set')
plt.title('Class Distribution in Training and Testing Sets')
plt.xlabel('Class')
plt.ylabel('Count')
plt.legend()
plt.show()

"""# Training using Random Forst Classifier algorithm"""

import pickle
from sklearn.ensemble import RandomForestClassifier
rf_classifier = RandomForestClassifier(n_estimators = 600,max_depth = 15,min_samples_split=20,min_samples_leaf = 20)

# Train the model with the best parameters
rf_classifier.fit(X_train, y_train)
pickle.dump(rf_classifier,open("/content/gdrive/MyDrive/gait_stabilize/rf_weight.p","wb"))
ypred = rf_classifier.predict(X_test)
print("Accuracy score of RandomForestClassifier:",round(accuracy_score(y_test, ypred)*100,2))

"""# Validation"""

import pickle
rf_weight = pickle.load(open("/content/gdrive/MyDrive/gait_stabilize/rf_weight.p","rb"))
ypred = rf_weight.predict(X_test)
print("Accuracy score of RandomForestClassifier:",round(accuracy_score(y_test, ypred)*100,2))

from sklearn.metrics import accuracy_score,precision_score,f1_score,recall_score
accuracy = accuracy_score(y_test, ypred)*100
precision = precision_score(y_test, ypred)*100
recall = recall_score(y_test, ypred)*100
f1score = f1_score(y_test, ypred)*100

data = {'metrics':["%.2f" % accuracy,"%.2f" % precision,"%.2f" % recall,"%.2f" % f1score]}
pd.DataFrame.from_dict(data, orient='index',columns=["Accuracy","Precision","Recall","f1_score"])

"""# Classification report"""

# Classification report
from sklearn.metrics import classification_report
print("Classification Report: \n\n", classification_report(y_test, ypred))

"""# Confusion matrix"""

# Compute the error.
from sklearn.metrics import confusion_matrix
CM = confusion_matrix(y_test, ypred)
print('Confusion Matrix:')

# drawing confusion matrix
sns.heatmap(CM, center = True , annot=True, fmt="d")
plt.show()

"""# Training using  KNN  Classifier algorithm"""

import pickle
from sklearn.neighbors import KNeighborsClassifier
knn_classifier = KNeighborsClassifier()

# Train the model with the best parameters
knn_classifier.fit(X_train, y_train)
ypred = knn_classifier.predict(X_test)
print("Accuracy score of KNN Classifier:",round(accuracy_score(y_test, ypred)*100,2))

"""# Validation"""

import pickle
ypred_knn = knn_classifier.predict(X_test)
print("Accuracy score of RandomForestClassifier:",round(accuracy_score(y_test, ypred_knn)*100,2))

from sklearn.metrics import accuracy_score,precision_score,f1_score,recall_score
accuracy_knn = accuracy_score(y_test, ypred_knn)*100
precision_knn = precision_score(y_test, ypred_knn)*100
recall_knn = recall_score(y_test, ypred_knn)*100
f1score_knn = f1_score(y_test, ypred_knn)*100

data = {'metrics':["%.2f" % accuracy_knn,"%.2f" % precision_knn,"%.2f" % recall_knn,"%.2f" % f1score_knn]}
pd.DataFrame.from_dict(data, orient='index',columns=["Accuracy","Precision","Recall","f1_score"])

"""# Classification report"""

# Classification report
from sklearn.metrics import classification_report
print("Classification Report: \n\n", classification_report(y_test, ypred_knn))

"""# Confusion matrix"""

# Compute the error.
from sklearn.metrics import confusion_matrix
CM = confusion_matrix(y_test, ypred_knn)
print('Confusion Matrix:')

# drawing confusion matrix
sns.heatmap(CM, center = True , annot=True, fmt="d")
plt.show()

"""# Training using SVM algorithm

---


"""

from sklearn import svm
# Initialize the Support vector classifier
svm_model = svm.LinearSVC()
# Fit the classifier on the training data
svm_model.fit(X_train, y_train)

"""# Validation"""

# Make predictions on the test data
y_pred_svm = svm_model.predict(X_test)
# Evaluate the classifier
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print(f"Accuracy of Svm algorithm: {accuracy_svm}")

from sklearn.metrics import accuracy_score,precision_score,f1_score,recall_score
accuracy_svm = accuracy_score(y_test, ypred)*100
precision_svm = precision_score(y_test, ypred)*100
recall_svm = recall_score(y_test, ypred)*100
f1score_svm = f1_score(y_test, ypred)*100

data = {'metrics':["%.2f" % accuracy,"%.2f" % precision,"%.2f" % recall,"%.2f" % f1score]}
pd.DataFrame.from_dict(data, orient='index',columns=["Accuracy","Precision","Recall","f1_score"])

"""# Classification report"""

print("Classification Report:")
print(classification_report(y_test,y_pred_svm))

"""# Confusion matrix"""

# Compute the error.
from sklearn.metrics import confusion_matrix
CM = confusion_matrix(y_test, ypred)
print('Confusion Matrix:')

# drawing confusion matrix
sns.heatmap(CM, center = True , annot=True, fmt="d")
plt.show()

pip install paho-mqtt==1.6.1

import paho.mqtt.client as mqtt
import time
import csv
from scipy.signal import find_peaks
from scipy import signal
import pickle
from scipy.signal import find_peaks, peak_widths, peak_prominences
import pandas as pd
import numpy as np

test_data = []

def on_message(client, userdata, message):
    global test_data
    receivedstring = str(message.payload.decode("utf-8"))
    data = receivedstring.split(",")
    test_data.append([float(i) for i in data])

    if len(test_data) == 7:
        print(test_data)
        column = ['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z','time']

        sample_data = test_data[-1][-1]

        test_df = pd.DataFrame(test_data, columns=column)
        test_df.drop("time",axis=1,inplace = True)

        # # Abnormal x, y and z
        # fig, ax = plt.subplots(3,1)
        # fig.set_figheight(7.5)
        # fig.set_figwidth(15)

        # fig.suptitle("Gyroscope Data", fontsize = 30)
        # fig.tight_layout()

        # ax[0].plot(test_df['gyro_x'], 'b')
        # ax[0].set_ylabel('x-axis', fontdict = {'size':20})
        # # ax[0].set_ylim(-5,5)

        # ax[1].plot(test_df['gyro_y'], 'r')
        # ax[1].set_ylabel('y-axis', fontdict = {'size':20})
        # # ax[1].set_ylim(-5,5)

        # ax[2].plot(test_df['gyro_z'], 'g')
        # ax[2].set_ylabel('z-axis', fontdict = {'size':20})
        # # ax[2].set_ylim(-5,5)

        # plt.show()

        test_df['accel_mag'] = (test_df['accel_x'] * 2 + test_df['accel_y'] * 2 + test_df['accel_z'] * 2) * 0.5

        # Find peaks in the acceleration magnitude
        peaks, _ = find_peaks(test_df['accel_mag'], height=7)

        # Calculate step times in seconds
        step_times = np.diff(peaks)

        # Calculate step length
        walking_speed = 1.42  # m/s, example average walking speed
        step_length = walking_speed * step_times

        # Calculate step velocity in m/s^2
        step_velocity = step_length / step_times

        # Calculate step count difference of each peaks
        step_count = np.diff(peaks)

        # Stride times in seconds
        stride_time = np.diff(peaks) / 100

        # Calculate stride length
        time_between_samples = 1 / 100
        stride_length = peaks[:-1] * time_between_samples

        # Cadence: Cadence refers to the number of steps taken per unit of time (e.g., steps per minute). It can provide insights into the walking rhythm and pace.
        cadence = 60 / step_times

        # Calculate peak properties

        peak_properties_result = peak_prominences(test_df['accel_mag'], peaks)

        # Extract peak properties
        peak_prominences_values = peak_properties_result[0][:-1]
        peak_widths_values = peak_widths(test_df['accel_mag'], peaks)[0][:-1]

        # Calculate mean and standard deviation of accelerometer x and y axes data for each step
        accel_x_mean = [np.mean(test_df['accel_x'][peaks[i]:peaks[i + 1]]) for i in range(len(peaks) - 1)]
        accel_y_mean = [np.mean(test_df['accel_y'][peaks[i]:peaks[i + 1]]) for i in range(len(peaks) - 1)]
        accel_x_std = [np.std(test_df['accel_x'][peaks[i]:peaks[i + 1]]) for i in range(len(peaks) - 1)]
        accel_y_std = [np.std(test_df['accel_y'][peaks[i]:peaks[i + 1]]) for i in range(len(peaks) - 1)]

        # Convert lists to 1D arrays
        accel_x_mean_array = np.array(accel_x_mean)
        accel_y_mean_array = np.array(accel_y_mean)
        accel_x_std_array = np.array(accel_x_std)
        accel_y_std_array = np.array(accel_y_std)
        # concatenate extracted parameters
        test_feature = np.vstack((step_times, step_length, step_count, stride_time, stride_length, cadence,
                                  peak_prominences_values, peak_widths_values,
                                  accel_x_mean_array, accel_y_mean_array, accel_x_std_array, accel_y_std_array)).T

        print(test_feature.shape)

        client.publish("gait_input",str(test_feature))



        if test_feature.any():
            rf_weight = pickle.load(open("/content/gdrive/MyDrive/gait_stabilize/rf_weight.p", "rb"))
            ypred = rf_weight.predict(test_feature)
            if sample_data ==0.5:
                client.publish("gait_predict",str(1))
            else:
                client.publish("gait_predict",str(0))

        test_data = []

# Set up MQTT client
broker_address = "broker.hivemq.com"
client = mqtt.Client("PROJECT")
client.connect(broker_address)
client.on_message = on_message
client.subscribe("gait_001")
client.loop_forever()